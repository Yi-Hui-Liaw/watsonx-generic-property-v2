{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Comment: Hello! I'm doing well, thanks for asking. How about you? Is there anything you'd like to chat about or ask? I'm here to help with any questions you might have.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import importlib\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "\n",
    "import prompt\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import (\n",
    "    WatsonxLLM,\n",
    ")\n",
    "\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.schema.embeddings import Embeddings\n",
    "from langchain.embeddings import HuggingFaceHubEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class MiniLML6V2EmbeddingFunctionLangchain(Embeddings):\n",
    "    MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return MiniLML6V2EmbeddingFunctionLangchain.MODEL.encode(texts).tolist()\n",
    "\n",
    "    def embed_query(self, query):\n",
    "        return MiniLML6V2EmbeddingFunctionLangchain.MODEL.encode([query]).tolist()[0]\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    model_id=ModelTypes.LLAMA_2_70B_CHAT,\n",
    "    credentials={\n",
    "        \"apikey\": os.getenv(\"IBM_API_KEY\"),\n",
    "        \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    },\n",
    "    params={\n",
    "        GenParams.DECODING_METHOD: \"sample\",\n",
    "        GenParams.MAX_NEW_TOKENS: 1024,\n",
    "        GenParams.TEMPERATURE: 0.1,\n",
    "        GenParams.RANDOM_SEED: 12345,\n",
    "    },\n",
    "    project_id=\"0353fa90-88c0-44d2-b6e7-ab143db3f01d\",\n",
    ")\n",
    "\n",
    "llm = WatsonxLLM(model=model)\n",
    "\n",
    "print(llm(\"hello how are you?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a different task than the one specified in the repository. Be sure to know what you're doing :)\n"
     ]
    }
   ],
   "source": [
    "# embeddings = MiniLML6V2EmbeddingFunctionLangchain()\n",
    "embeddings = HuggingFaceHubEmbeddings(\n",
    "    repo_id=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    task=\"feature-extraction\",\n",
    "    huggingfacehub_api_token=os.getenv(\"HF_TOKEN\"),\n",
    ")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "text_files = [\n",
    "    \"../data/qa/the-connaught-one.txt\",\n",
    "    \"../data/qa/the-minh.txt\",\n",
    "    \"../data/qa/residensi-zig.txt\",\n",
    "]\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=open(x, encoding=\"utf-8\").read(),\n",
    "        metadata={\"filename\": pathlib.Path(x).stem},\n",
    "    )\n",
    "    for x in text_files\n",
    "]\n",
    "docs = text_splitter.split_documents(docs)\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "db.save_local(\"../code-engine/db\")\n",
    "db = FAISS.load_local(\"../code-engine/db\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(prompt)\n",
    "from prompt import QUESTION_TEMPLATE\n",
    "_ = shutil.copy(\"prompt.py\", \"../code-engine/app/prompt.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_DOCS = 5\n",
    "\n",
    "def build_prompt(question):\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "    BOS, EOS = \"<s>\", \"</s>\"\n",
    "    DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "    You are an expert Q&A system that is trusted around the world. Always answer the query using the provided context information, and not prior knowledge.\n",
    "    Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \n",
    "    \"\"\"\n",
    "    prompt = \"\".join([B_SYS, DEFAULT_SYSTEM_PROMPT, E_SYS, question]).strip()\n",
    "    return f\"{BOS}{B_INST} {prompt} {E_INST}\"\n",
    "\n",
    "\n",
    "def chat(messages, property=None):\n",
    "    question = messages[-1][\"u\"]\n",
    "    search_results = db.similarity_search(question, k=K_DOCS)\n",
    "    if property:\n",
    "        search_results = list(filter(lambda x: x.metadata[\"filename\"] == property, search_results))\n",
    "    context = \" \".join([x.page_content for x in search_results])\n",
    "    question = QUESTION_TEMPLATE.replace(\"{{context}}\", context).replace(\n",
    "        \"{{question}}\", question\n",
    "    )\n",
    "    prompt = build_prompt(question)\n",
    "    assistant = llm(prompt).strip().replace(\"•\", \"*\").replace(\"```\", \"\")\n",
    "    source = \"\"\n",
    "    if \"I do not know\" not in assistant:\n",
    "        source = \"\\n\\nSource:\\n- \" + \"\\n- \".join(\n",
    "            dict.fromkeys([x.metadata[\"filename\"] for x in search_results[:1]])\n",
    "        )\n",
    "    messages[-1][\"u\"] = question\n",
    "    messages.append({\"a\": f\"{assistant}{source}\"})\n",
    "    return messages, context, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What are the nearby ammenties?\n",
      "A:\n",
      "The nearby amenities include a 100m connected MRT station, a dedicated urban yard, a signature adaptive home, a parcel room, a home-work zone, and 5,850m² of green area.\n",
      "\n",
      "Source:\n",
      "- the-connaught-one\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# valid property: the-connaught-one, the-minh, residensi-zig\n",
    "\n",
    "property = \"the-connaught-one\"\n",
    "\n",
    "questions = [\n",
    "    \"What are the nearby ammenties?\",\n",
    "]\n",
    "\n",
    "messages = []\n",
    "for q in questions:\n",
    "    messages.append({\"u\": q})\n",
    "    messages, context, prompt = chat(messages, property)\n",
    "    a = messages[-1][\"a\"]\n",
    "    print(f\"Q: {q}\")\n",
    "    print(\"A:\")\n",
    "    print(f\"{a}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
