{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c05cfa1-5139-4e86-8ffe-1ad9a794bfc1",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "This notebook walks through the logic behind the chatbot application\n",
    "\n",
    "# Sections\n",
    "\n",
    "- Section 1: High-Level Overview of the Application\n",
    "- Section 2: Data\n",
    "- Section 3: Models\n",
    "- Section 4: Prompts\n",
    "- Section 5: Application Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a4c988e-05f1-498d-983c-ad64253e609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import importlib\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import re\n",
    "import action\n",
    "import prompt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import (\n",
    "    WatsonxLLM,\n",
    ")\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from hf_hub import HuggingFaceHubEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5068776-6d25-4e8e-9c02-647dc82a3035",
   "metadata": {},
   "source": [
    "# Section 1: High-Level Overview\n",
    "\n",
    "This generative AI-powered application is made up of the following 4 core components:\n",
    "\n",
    "- Input Data\n",
    "- Models:\n",
    "  - LLMs: Models (LLAMA, FLAN, GRANITE) which take text as input and return text as output. This is the \"generative AI\" component of our application\n",
    "  - Search Model: Models to help us to search documents for information that is relevant to the customer question. In the pilot code below, we used simple semantic search. In production, elastic search is recommended.\n",
    "- Prompts: Texts which are provided as input to the LLMs, usually taking the form of instructions. \n",
    "- Application Logic: The code written to chain the different LLM inputs and outputs together to build a full-fledged application\n",
    "\n",
    "Displayed below is a diagram which depicts how the models, prompts, and application logic are tied together to form the full chatbot application. We've split the diagram into 2 solutions: the first is the main chat application, and the second is for FAQ extraction. The main solution is solution 1. \n",
    "\n",
    "![](arch4.jpg)\n",
    "\n",
    "For solution 1, reading from left to right, here is how a user's question is processed by the GenAI application. \n",
    "\n",
    "* First, the user asks the question in watson assistant.\n",
    "* The first prompt that we use is the \"routing prompt\" which is used to determine whether the user question should be answered by referring to data from a transactional database or by referring to unstructured data\n",
    "* The second prompt that we use is the \"property name prompt\" which is used to determine whether the property name is mentioned in the user question\n",
    "* Depending on the result of the first prompt, the logic diverges into one of two pipelines.\n",
    "  * RAG pipeline: In the RAG pipeline, we find relevant documents using our search mechanism, then pass the documents and the third prompt (RAG prompt) to the LLM. The LLM uses the retrieved relevant documents to answer the user query. We use the fourth prompt (custom response prompt\" to find images which are relevant to the response obtained from the RAG prompt.\n",
    "  * SQL pipeline: In the SQL pipeline, we use the fifth prompt (SQL prompt) to convert the user question to an SQL query, then we query the SQL database. Finally, the output of the SQL query is converted back to natural language using the \"direct answer prompt\"\n",
    "* Finally output is relayed back to the user in watson assistant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8b6513-0664-410e-a0e7-01642e85631f",
   "metadata": {},
   "source": [
    "# Section 2: Input Data\n",
    "\n",
    "In the pilot, we took as input some structured table data exported from SalesForce, and some unstructured text data about several UEM properties. In the cell below, we read that data and do some preprocessing.\n",
    "\n",
    "For the structured data, we add the Excel data into an SQL database.\n",
    "For the unstructured data, we read it from the text file and split it up into chunks.\n",
    "\n",
    "Important Note: In the production case, the data ingestion process will look quite different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ac65e92-1120-40ef-a5df-4065a350b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Structured Data\n",
    "engine = create_engine(\"sqlite:///database.db\", echo=False)\n",
    "data = pd.read_excel(\"../../backend/data/sql/zig-minh-connaught-sample.xlsx\", sheet_name=None)\n",
    "for k, v in data.items():\n",
    "    v.columns = [x.replace(\" \", \"_\") for x in v.columns]\n",
    "    table = k.split(\" \")[0]\n",
    "    v.to_sql(table, con=engine, index=False, if_exists=\"replace\")\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "\n",
    "db = SessionLocal()\n",
    "with db.connection().engine.connect() as conn:\n",
    "    connaught = pd.read_sql(text(\"SELECT * from connaught\"), conn)\n",
    "    minh = pd.read_sql(text(\"SELECT * from minh\"), conn)\n",
    "    zig = pd.read_sql(text(\"SELECT * from zig\"), conn)\n",
    "\n",
    "\n",
    "# Unstructured Data\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "text_files = [\n",
    "    \"../data/qa/the-connaught-one.txt\",\n",
    "    \"../data/qa/the-minh.txt\",\n",
    "    \"../data/qa/residensi-zig.txt\",\n",
    "]\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=open(x, encoding=\"utf-8\").read(),\n",
    "        metadata={\"filename\": pathlib.Path(x).stem},\n",
    "    )\n",
    "    for x in text_files\n",
    "]\n",
    "docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724fcbf8-c61c-4834-83a7-89b8c09f3975",
   "metadata": {},
   "source": [
    "# Section 3.1: LLM Models\n",
    "\n",
    "In the cell below, we instantiate the LLMs which we will use to build out the application.\n",
    "Notice that this is where we authenticate to watsonx, and are able to access different LLMs i.e. FLAN, LLAMA and GRANITE.\n",
    "We will use these as the building blocks of our AI application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbb599c8-fae4-4531-b890-b88f47be926d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolearitranair/Documents/Projects/uem/watsonx-uem-sunrise/uem-20240130/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/nicolearitranair/Documents/Projects/uem/watsonx-uem-sunrise/uem-20240130/lib/python3.10/site-packages/ibm_watson_machine_learning/foundation_models/utils/utils.py:111: LifecycleWarning: Model 'ibm/granite-13b-instruct-v1' is in constricted state from 2024-01-11 until 2024-04-11. IDs of alternative models: ibm/granite-13b-instruct-v2. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
      "  warnings.warn(default_warning_template.format(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import importlib\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import prompt\n",
    "from dotenv import load_dotenv\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import (\n",
    "    WatsonxLLM,\n",
    ")\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceHubEmbeddings\n",
    "from langchain.schema.embeddings import Embeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "engine = create_engine(\"sqlite://\", echo=False)\n",
    "\n",
    "MODELS = [\n",
    "    ModelTypes.FLAN_T5_XXL,\n",
    "    ModelTypes.LLAMA_2_70B_CHAT,\n",
    "    \"meta-llama/llama-3-70b-instruct\",\n",
    "]\n",
    "\n",
    "MODELS = {\n",
    "    x: WatsonxLLM(\n",
    "        model=Model(\n",
    "            model_id=x,\n",
    "            credentials={\n",
    "                \"apikey\": os.getenv(\"IBM_API_KEY\"),\n",
    "                \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "            },\n",
    "            params={\n",
    "                GenParams.DECODING_METHOD: \"greedy\",\n",
    "                GenParams.MAX_NEW_TOKENS: 300,\n",
    "                GenParams.TEMPERATURE: 0,\n",
    "                GenParams.RANDOM_SEED: 12345,\n",
    "                GenParams.STOP_SEQUENCES: [\"\\n\\n\"],\n",
    "            },\n",
    "            project_id=os.getenv(\"PROJECT_ID\"),\n",
    "        )\n",
    "    )\n",
    "    for x in MODELS\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3b2ee89-6c5b-4499-ba4c-8eb5d8ea79a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolearitranair/Documents/Projects/uem/watsonx-uem-sunrise/uem-20240130/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am fine\n"
     ]
    }
   ],
   "source": [
    "#this is how a model works\n",
    "print(MODELS[ModelTypes.FLAN_T5_XXL](\"hello how are you?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a587329-8bae-4eaf-a40e-00d195843d00",
   "metadata": {},
   "source": [
    "# Section 3.2: Search Model\n",
    "\n",
    "In the pilot, we used simple semantic search using a HuggingFace model as shown below. In production, elastic search is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eca45ff-07cb-4ae5-a888-837c079fa3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceHubEmbeddings(\n",
    "    model=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    task=\"feature-extraction\",\n",
    "    huggingfacehub_api_token=os.getenv(\"HF_TOKEN\"),\n",
    ")\n",
    "\n",
    "vdb = FAISS.from_documents(docs, embeddings)\n",
    "vdb.save_local(\"../../backend/vdb\")\n",
    "vdb = FAISS.load_local(\"../../backend/vdb\", embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53d6dae-4ed3-4601-bf51-29e608464e11",
   "metadata": {},
   "source": [
    "# Section 4: Defining the Prompts\n",
    "\n",
    "The prompts are currently maintained in prompt.py, which you can find in notebook/prompt.py.\n",
    "\n",
    "In the below cell we display the RAG prompt after importing it.\n",
    "\n",
    "You can explore by importing other prompts.\n",
    "\n",
    "The prompts are described in Section 1, so they will not be repeated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f5d21ed-0c7d-4a20-9dce-1d7967d9f93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG prompt: \n",
      " \n",
      "Context information is below.\n",
      "---------------------\n",
      "{{context}}\n",
      "---------------------\n",
      "\n",
      "Given only the context information and no prior knowledge, answer the query in a brief and concise manner using only one sentence.  \n",
      "\n",
      "Avoid statements like 'Based on the context, ...' or 'According to the provided context ...', or anything along those lines.\n",
      "\n",
      "If you don't know the answer to a query, say \"I do not know\".\n",
      "\n",
      "If the user did not ask a question, you should reply accordingly in a conversational manner. \n",
      "\n",
      "Query: {{question}} \n",
      " \n",
      "Response: \n",
      "\n",
      " \n",
      "************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import prompt\n",
    "\n",
    "print(\"RAG prompt: \\n\", prompt.QUESTION_TEMPLATE, \"\\n************************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ba35f4-e954-44ff-9fff-0b6cb93acde7",
   "metadata": {},
   "source": [
    "# Section 5: Application Logic\n",
    "\n",
    "The primary application logic utilizes several \"action\" functions which are imported . You can think of the \"action\" functions as the types of questions that can be handled by the chatbot. Recall that in the architecture diagram defined in the first cell, depending on the routing prompt output, we follow the \"RAG pipeline\" or the \"SQL pipeline. The 2 primary action functions \"property_specific_general_query\" and \"transactional_query\" define the RAG pipeline and SQL pipeline respectively.\n",
    "\n",
    "Note that we do not yet have a function, nor the input data to handle  non-property specific RAG queries. It is implemented as a placeholder function called \"general_query\" which you can refer to in action.py\n",
    "\n",
    "![](arch5.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e89fc554-e01d-4962-a939-dab6756d0fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL PIPELINE: transactional_query\n",
      "********************************************************************************************************************\n",
      "def transactional_query(params: ActionParams):\n",
      "    question = params[\"question\"]\n",
      "    models = params[\"models\"]\n",
      "    db = params[\"db\"]\n",
      "    # if params[\"property\"] is None:\n",
      "    #     return {\n",
      "    #         \"generated_text\": \"Sounds like you're asking a question about a property. Kindly specify a valid property name so that I can answer this question correctly.\",\n",
      "    #         \"custom_response\": {},\n",
      "    #     }\n",
      "    if params[\"property\"] is not None and params[\"detected_property_name\"] == \"NONE\":\n",
      "        prop_replace = params[\"property\"]\n",
      "        question = f\"For {prop_replace}, {question}\"\n",
      "    prompt = build_prompt(\n",
      "        SQL_TEMPLATE.replace(\"{{question}}\", question), SQL_SYSTEM_PROMPT\n",
      "    )\n",
      "    sql = models[ModelTypes.LLAMA_2_70B_CHAT](prompt).strip()\n",
      "    if \"```\" in sql:\n",
      "        sql = re.search(\"```\\n([\\S\\s]*);\\n\", sql).group(1)\n",
      "    if \"UNION\" in sql:\n",
      "        sql = re.sub(\"\\n\", \" \", sql)\n",
      "        sql = re.sub(\"SELECT ([a-zA-z\\(\\*\\)]+) FROM ([a-zA-z]+)\", r\"SELECT \\1, '\\2' as property_name FROM \\2\", sql.strip())\n",
      "        print(f\"Generated SQL Query: {sql}\")\n",
      "        with db.connection().engine.connect() as conn:\n",
      "            results = pd.read_sql(text(sql), conn)\n",
      "            # for rn in results.columns:\n",
      "            #     if \"Price\" in rn:\n",
      "            #         results.loc[:, rn] = results[rn].map('{:,.0f}'.format)\n",
      "        ordered_tb_names = results[\"property_name\"][:5]\n",
      "        # results = results.drop(columns=[\"property_name\"])\n",
      "    else:\n",
      "        print(f\"Generated SQL Query: {sql}\")\n",
      "        with db.connection().engine.connect() as conn:\n",
      "            results = pd.read_sql(text(sql), conn)\n",
      "            # for rn in results.columns:\n",
      "            #     if \"Price\" in rn:\n",
      "            #         results.loc[:, rn] = results[rn].map('{:,.0f}'.format)\n",
      "        ordered_tb_names = None\n",
      "    if results.shape == (1, 1):\n",
      "        answer = str(results.iloc[0, 0])\n",
      "        prompt = DIRECT_ANSWER_TEMPLATE_v1.replace(\"{{answer}}\", answer).replace(\n",
      "            \"{{question}}\", question\n",
      "        )\n",
      "        generated_text = models[ModelTypes.FLAN_T5_XXL](prompt).strip()\n",
      "    elif results.shape[1] <= 2:\n",
      "        generated_text = results.head(5).to_json(orient=\"records\")\n",
      "        if \"UNION\" in sql:\n",
      "            if len(set(list(results[\"property_name\"])))==3 and results.shape[0]==3:\n",
      "                queried = \", \".join(ordered_tb_names)\n",
      "                generated_text += f\"\\nInfo provided is for the following properties: {queried}\"\n",
      "                prompt = DIRECT_ANSWER_TEMPLATE_v2.replace(\"{{answer}}\", generated_text).replace(\n",
      "                \"{{question}}\", question\n",
      "            )\n",
      "                generated_text = models[ModelTypes.FLAN_T5_XXL](prompt).strip()\n",
      "            else:\n",
      "                generated_text = \"Here are some relevant results that we found: \\n\" + generated_text\n",
      "        else:\n",
      "            prompt = DIRECT_ANSWER_TEMPLATE_v2.replace(\"{{answer}}\", generated_text).replace(\n",
      "                \"{{question}}\", question\n",
      "            )\n",
      "            generated_text = models[ModelTypes.FLAN_T5_XXL](prompt).strip()\n",
      "        if results.shape[0]>5:\n",
      "            table = re.findall(\"FROM ([a-z]+)\", sql)\n",
      "            table = \", \".join([reference_urls1[t] for t in table])\n",
      "            generated_text += f\"\\nPlease note that there are more than 5 relevant results. Please refer to {table} for more information\"\n",
      "\n",
      "    elif results.shape[0] == 1:\n",
      "        results = results.transpose()\n",
      "        generated_text = results.head(5).to_json(orient=\"records\")\n",
      "        if \"UNION\" in sql:\n",
      "            if len(set(list(results[\"property_name\"])))==3 and results.shape[0]==3:\n",
      "                queried = \", \".join(ordered_tb_names)\n",
      "                generated_text += f\"\\nInfo provided is for the following properties: {queried}\"\n",
      "                prompt = DIRECT_ANSWER_TEMPLATE_v2.replace(\"{{answer}}\", generated_text).replace(\n",
      "                \"{{question}}\", question\n",
      "            )\n",
      "                generated_text = models[ModelTypes.FLAN_T5_XXL](prompt).strip()\n",
      "            else:\n",
      "                generated_text = \"Here are some relevant results that we found: \\n\" + generated_text\n",
      "        else:\n",
      "            prompt = DIRECT_ANSWER_TEMPLATE_v2.replace(\"{{answer}}\", generated_text).replace(\n",
      "                \"{{question}}\", question\n",
      "            )\n",
      "            generated_text = models[ModelTypes.FLAN_T5_XXL](prompt).strip()\n",
      "        if results.shape[0]>5:\n",
      "            table = re.findall(\"FROM ([a-z]+)\", sql)\n",
      "            table = \", \".join([reference_urls1[t] for t in table])\n",
      "            generated_text += f\"\\nPlease note that there are more than 5 relevant results. Please refer to {table} for more information\"\n",
      "\n",
      "    else:\n",
      "        print(results)\n",
      "        generated_text = \"Sorry, that type of query is not currently supported - you need to be more specific\"\n",
      "\n",
      "    generated_text = generated_text.replace(\"residensi-zig\", \"Residensi Zig\")\n",
      "    generated_text = generated_text.replace(\"the-connaught-one\", \"The Connaught One\")\n",
      "    generated_text = generated_text.replace(\"the-minh\", \"The MINH\")\n",
      "    generated_text = generated_text.replace(\"zig\", \"Residensi Zig\")\n",
      "    generated_text = generated_text.replace(\"connaught\", \"The Connaught One\")\n",
      "    generated_text = generated_text.replace(\"minh\", \"The MINH\")\n",
      "    for x in reference_urls2:\n",
      "        generated_text = generated_text.replace(x, reference_urls2[x])\n",
      "\n",
      "    generated_text = postprocess_numeric(generated_text)\n",
      "    return {\"generated_text\": generated_text, \"custom_response\": {}}\n",
      "\n",
      "********************************************************************************************************************\n",
      "RAG PIPELINE: property_specific_general_query\n",
      "********************************************************************************************************************\n",
      "def property_specific_general_query(params: ActionParams):\n",
      "    question = params[\"question\"]\n",
      "    models = params[\"models\"]\n",
      "    vdb = params[\"vdb\"]\n",
      "    property = params[\"property\"]\n",
      "    if params[\"property\"] is None:\n",
      "        return {\n",
      "            \"generated_text\": \"Sounds like you're asking a question about a property. Kindly specify a valid property name so that I can answer this question correctly.\",\n",
      "            \"custom_response\": {},\n",
      "        }\n",
      "    search_results = vdb.similarity_search(question, k=3)\n",
      "    if property:\n",
      "        print(f\"Property: {property}\")\n",
      "        search_results = list(\n",
      "            filter(lambda x: x.metadata[\"filename\"] == property, search_results)\n",
      "        )\n",
      "    context = \" \".join([x.page_content for x in search_results])\n",
      "    prompt = build_prompt(\n",
      "        QUESTION_TEMPLATE.replace(\"{{context}}\", context).replace(\n",
      "            \"{{question}}\", question\n",
      "        ),\n",
      "        DEFAULT_SYSTEM_PROMPT,\n",
      "    )\n",
      "    generated_text = models[ModelTypes.LLAMA_2_70B_CHAT](prompt).strip()\n",
      "    prompt = CUSTOM_RESPONSE_TEMPLATE.replace(\"{{description}}\", generated_text)\n",
      "    custom_response = {}\n",
      "    generated_custom_response = models[ModelTypes.GRANITE_13B_INSTRUCT](prompt).strip()\n",
      "    if \"None\" not in generated_custom_response:\n",
      "        try:\n",
      "            custom_response[\"images\"] = json.loads(\n",
      "                generated_custom_response.replace(\"'\", '\"')\n",
      "            )\n",
      "            custom_response[\"images\"] = [\n",
      "                x for x in custom_response[\"images\"] if x in IMAGES\n",
      "            ]\n",
      "        except Exception:\n",
      "            pass\n",
      "\n",
      "    return {\"generated_text\": generated_text, \"custom_response\": custom_response}\n",
      "\n",
      "********************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "import action\n",
    "import inspect\n",
    "\n",
    "print(\"SQL PIPELINE: transactional_query\")\n",
    "print(\"********************************************************************************************************************\")\n",
    "print(inspect.getsource(action.transactional_query))\n",
    "print(\"********************************************************************************************************************\")\n",
    "\n",
    "print(\"RAG PIPELINE: property_specific_general_query\")\n",
    "print(\"********************************************************************************************************************\")\n",
    "print(inspect.getsource(action.property_specific_general_query))\n",
    "print(\"********************************************************************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3823d50-aacc-43ef-a417-aa14032940ee",
   "metadata": {},
   "source": [
    "In the cell below, you can see the \"generate\" function which ties the data, models, actions, and prompts into a GenAI Q&A bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9afcd974-14c5-44d7-9322-843f5d85bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from action import ACTIONS\n",
    "from prompt import DEFAULT_SYSTEM_PROMPT, ROUTING_TEMPLATE, build_prompt, PROPERTY_TEMPLATE\n",
    "\n",
    "    \n",
    "def generate(generate_request):\n",
    "    params = {\n",
    "    \"models\": MODELS,\n",
    "    \"db\": db,\n",
    "    \"vdb\": vdb,\n",
    "    \"property\": None, \n",
    "    }\n",
    "    k_docs = generate_request[\"k_docs\"]\n",
    "    wa_property = generate_request[\"current_page\"]\n",
    "    question = generate_request[\"question\"]\n",
    "    prompt = ROUTING_TEMPLATE.replace(\"{{question}}\", question)\n",
    "    action_output = int(params[\"models\"][ModelTypes.FLAN_T5_XXL](prompt).strip())\n",
    "    if action_output in [1, 3]:\n",
    "        property = None\n",
    "        property_name = params[\"models\"][ModelTypes.FLAN_T5_XXL](\n",
    "            PROPERTY_TEMPLATE.replace(\"{{question}}\", question)\n",
    "        ).strip()\n",
    "        if wa_property is not None:\n",
    "            property = wa_property\n",
    "        if property_name != \"NONE\":\n",
    "            property = property_name\n",
    "        params.update(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"property\": property,\n",
    "                \"detected_property_name\": property_name\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        params.update({\"question\": question, \"property\": None, \"detected_property_name\": \"NONE\"})\n",
    "    print(f\"Question: {question} - [Action: {action_output}]\")\n",
    "    generated_text, custom_response = ACTIONS[action_output](params).values()\n",
    "    return {\"generated_text\": generated_text, \"custom_response\": custom_response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c3da2ab-f99d-4acb-ac9b-db959ab9b45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Give me the minimum price of available units? - [Action: 1]\n",
      "Generated SQL Query: SELECT min(Price)\n",
      "FROM minh\n",
      "WHERE Status = 'Available';\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'generated_text': 'The MINH has a minimum price of 1,404,800.',\n",
       " 'custom_response': {}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test on a question\n",
    "wa_request = {\n",
    "    \"k_docs\": 3,\n",
    "    \"current_page\": \"the-minh\",\n",
    "    \"question\": \"Give me the minimum price of available units?\"\n",
    "}\n",
    "generate(wa_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062270fd-7c8c-46a4-9da2-2394b932df26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uem-20240130",
   "language": "python",
   "name": "uem-20240130"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
