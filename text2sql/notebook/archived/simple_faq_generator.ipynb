{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method 1: generate questions and answers directly\n",
    "def generate_questions_and_answers_type_1(docs):\n",
    "\n",
    "    questions_and_answers_all = []\n",
    "    model = Model(\n",
    "        model_id=ModelTypes.LLAMA_2_70B_CHAT,\n",
    "        credentials={\n",
    "            \"apikey\": os.getenv(\"IBM_API_KEY\"),\n",
    "            \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "        },\n",
    "        params={\n",
    "            GenParams.DECODING_METHOD: \"greedy\",\n",
    "            GenParams.MAX_NEW_TOKENS: 800,\n",
    "            GenParams.TEMPERATURE: 0,\n",
    "            GenParams.STOP_SEQUENCES:[\"]\"],\n",
    "        GenParams.RANDOM_SEED: 12345,\n",
    "    },\n",
    "    project_id=os.getenv(\"PROJECT_ID\"),\n",
    ")\n",
    "\n",
    "    llm = WatsonxLLM(model=model)\n",
    "    for i in tqdm(docs):\n",
    "        prompt = f\"\"\"\n",
    "\n",
    "        Context: {i}\n",
    "\n",
    "        Query: Generate 3 most likely user questions and the correct answers based on the context information above. Use your own words and do not repeat the questions that are provided in the context information. Return the answer in as a list of JSONs with \"question\" and \"answer\" as the keys.\n",
    "\n",
    "        Response: \n",
    "        \n",
    "        \"\"\"\n",
    "        questions_and_answers_all.append(llm(prompt))\n",
    "\n",
    "    return questions_and_answers_all\n",
    "\n",
    "#max out at 5 for now so demo does not take too long to run\n",
    "qaa_method_1 = generate_questions_and_answers_type_1(docs_ava[:5])\n",
    "qaa2_method_1 = generate_questions_and_answers_type_1(docs_multi_prop[:20])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
